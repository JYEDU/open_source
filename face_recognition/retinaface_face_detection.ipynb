{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection with Retinaface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] [[Paper](https://arxiv.org/abs/1905.00641)] [[Github](https://github.com/biubug6/Pytorch_Retinaface.git)] Deng, Jiankang and Guo, Jia and Yuxiang, Zhou and Jinke Yu and Irene Kotsia and Zafeiriou, Stefanos, \"RetinaFace: Single-stage Dense Face Localisation in the Wild\", arxiv, 2019\n",
    "\n",
    "[2] [[Paper](https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_WIDER_FACE_A_CVPR_2016_paper.html)] [[Dataset](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html)] Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang, \"WIDER FACE: A Face Detection Benchmark\", Proceedings of the IEEE conference on computer vision and pattern recognition, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "- Create virtual environment and install library\n",
    "    - conda create -n retinaface python=3.8\n",
    "    - conda activate retinaface\n",
    "    - pip install torch torchvision\n",
    "    - pip install tqdm PyYAML scipy mxnet opencv-python\n",
    "    - pip install matplotlib python-time torchsummary\n",
    "\n",
    "- Download the Retinaface & WIDERFACE dataset & annotations \n",
    "    - [Retinaface github & Annotations & Pretrained model](https://github.com/biubug6/Pytorch_Retinaface?tab=readme-ov-file#data)\n",
    "    - [WIDERFACE datset](http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html)\n",
    "\n",
    "- Place the files as follows:\n",
    "\n",
    "    ```\n",
    "    Pytorch_Retinaface-master                   # from Retinaface git\n",
    "    └── dataset                                 # Dataset\n",
    "        └── widerface\n",
    "            └── WIDER_train\n",
    "                └── images/\n",
    "                └── train_label.txt\n",
    "            └── WIDER_val/\n",
    "                └── images/\n",
    "                └── val_label.txt\n",
    "    └── weights                                 # pretrained weights\n",
    "        └── mobilenet0.25_Final.pth\n",
    "\n",
    "    ```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cfg_mnet, cfg_re50\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretinaface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetinaFace\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_cpu_nms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_cpu_nms\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "# 라이브러리 삽입\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.box_utils import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "weight_path = \"mobilenet0.25_Final.pth\" #.pth\n",
    "test_path =                             # test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "cfg = cfg_mnet # mobile0.25 (cfg_mnet) or resnet50 (cfg_re50)\n",
    "resize = 1\n",
    "confidence_threshold = 0.02\n",
    "top_k = 5000\n",
    "nms_threshold = 0.4\n",
    "keep_top_k = 750\n",
    "vis_thres = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model=RetinaFace(cfg, phase = 'test').to(device)\n",
    "model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "# 평가 모드로 전환하기\n",
    "model.eval()\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retinaface Inference 함수\n",
    "def retinaface_inf(test_img, model):\n",
    "    img = np.float32(test_img)\n",
    "    im_height, im_width, _ = img.shape\n",
    "\n",
    "    scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "    img -= (104, 117, 123)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    scale = scale.to(device)\n",
    "\n",
    "    tic = time.time()\n",
    "    loc, conf, landms = model(img)  # forward pass\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    boxes = boxes * scale / resize\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "\n",
    "    # ignore low scores\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    scores = scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    order = scores.argsort()[::-1][:top_k]\n",
    "    boxes = boxes[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    # do NMS\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    dets = dets[keep, :]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    dets = dets[:keep_top_k, :]\n",
    "\n",
    "    fps_ = round(1/(time.time() - tic), 2)\n",
    "    for b in dets:\n",
    "        if b[4] < vis_thres:\n",
    "            continue\n",
    "        b = list(map(int, b))\n",
    "        cv2.rectangle(test_img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 4)\n",
    "    cv2.putText(test_img, \"retinaface\", (410,70),cv2.FONT_HERSHEY_DUPLEX, 1.5,(255,0,0), thickness=3, lineType=cv2.LINE_AA)\n",
    "    cv2.putText(test_img, \"fps : \"+str(fps_), (5,70),cv2.FONT_HERSHEY_DUPLEX, 1.5,(0,0,255), thickness=3, lineType=cv2.LINE_AA)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test image\n",
    "test_img = cv2.imread(test_path)\n",
    "\n",
    "retina_img = retinaface_inf(test_img, model)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
